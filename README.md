# Eleuther experiments in general intelligence
### Learning from human feedback 

Large language models learn a lot of very useful information about the world, we are experimenting with human preferences to steer the models

The task of summarisation acts as a good testing ground for comparing methods as OpenAI have set some baselines and released their datasets [here](https://github.com/openai/summarize-from-feedback).
# Links



Download comparisons from OpenAI `azcopy copy "https://openaipublic.blob.core.windows.net/summarize-from-feedback/dataset/*" . --recursive`.

[OpenAI's Dataset explorer](https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/)

(not currently working)
`https://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered` and `https://openaipublic.blob.core.windows.net/summarize-from-feedback/datasets/tldr_3_filtered_queries` also host OpenAI's filtered verson of the dataset [TL;DR dataset](https://zenodo.org/record/1168855) by Syed, Shahbaz, Voelske, Michael, Potthast, Martin, & Stein, Benno (2018). It is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode). 