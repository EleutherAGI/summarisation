{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TLDRDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "ds = TLDRDataset('data/tldr-filtered.json', tokenizer)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (text, summary_length) in enumerate(dl):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, padding=True, truncation=True, return_length = True, max_length = 512, return_tensors = 'pt')\n",
    "total_length = inputs.pop('length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers.testing_utils import CaptureLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default-aacabb4e0f3c1bc0 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/kip/.cache/huggingface/datasets/json/default-aacabb4e0f3c1bc0/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/kip/.cache/huggingface/datasets/json/default-aacabb4e0f3c1bc0/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset('json', field='data', data_files = {'test': './data/tldr-filtered-test.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = datasets[\"test\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        text = [content + ' TLDR:' + summary for content, summary in zip(examples['content'], examples['summary'])]\n",
    "        output = tokenizer(text, return_length = True)\n",
    "        \n",
    "        summary_lengths = tokenizer(examples['summary'], return_length = True)['length']\n",
    "        total_lengths = output.pop(\"length\")\n",
    "        # for item in tokenize batch\n",
    "        output['mask'] = []\n",
    "        for i in range(len(summary_lengths)):\n",
    "            output['mask'].append([0 for _ in range(total_lengths[i])])\n",
    "            for j in range(total_lengths[i] - summary_lengths[i], total_lengths[i]):\n",
    "                output['mask'][i][j] = 1\n",
    "    # clm input could be much much longer than block_size\n",
    "    if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "        tok_logger.warning(\n",
    "            \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "        )\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in tokenized_datasets['test']:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 1024\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    \n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "python midtune.py \\\n",
    "    --model_name_or_path distilgpt2 \\\n",
    "    --train_file ../../data/tldr/tldr-filtered-train.json \\\n",
    "    --validation_file ../../data/tldr/tldr-filtered-test.json \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir /media/external_usb/kip/models/distilgpt2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed --num_gpus=2 midtune.py \\\n",
    "--deepspeed ds_config.json \\\n",
    "--model_name_or_path gpt2-xl \\\n",
    "--train_file ../../data/tldr/tldr-filtered-train.json \\\n",
    "--validation_file ../../data/tldr/tldr-filtered-test.json \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--fp16 \\\n",
    "--overwrite_cache \\\n",
    "--evaluation_strategy=\"steps\" \\\n",
    "--output_dir finetuned \\\n",
    "--eval_steps 200 \\\n",
    "--num_train_epochs 1 \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--per_device_train_batch_size 1\n",
    "\n",
    "#Data storage location\n",
    "#/home/aleph/.cache/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "8\n",
      "11\n",
      "14\n",
      "17\n",
      "20\n",
      "23\n",
      "26\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "acc_steps = 3\n",
    "\n",
    "for i in range(30):\n",
    "    if (i + 1) % acc_steps == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
    "for i, (s, t) in enumerate(zip(summary_length, total_length)):\n",
    "        mask[i][t - s - 1 : t - 1] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34094,  3454,  2194,  1869,    13])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels[0, mask[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series of unrelated fucked up things reveals that I'm a very selfish person.\n",
      "Long one, \n",
      " I was in Walmart, being chased by a man whom I knew was going to rape me/cause me serious bodily harm. Said Walmart became a maze and was increasingly hard to navigate, then I come upon my ex boyfriend in the yarn section (which I'm not sure exists in Walmart stores) and he refused to help me. He said, \"You deserve what is happening to you\". So I'm running out of breath, feeling terrified, knowing I'm going to be caught when I see the door and run outside. \n",
      " It's raining, and very dark out. Two of my professors are out in the parking lot, one is crying and staring off into the distance. I follow her gaze to see this terrible scene: A bridge over a gaping ravine has collapsed and cars are still driving off the edge and crashing into the river beneath. People are dying right before my eyes, screaming, crying. My ears are filled with a terrible screeching sound louder than any other sobbing and I'm suddenly underneath the bridge looking up. \n",
      " I turn around and there's a little girl laying on the river-bank, she's making the screeching sound. I can see blood everywhere but begin to panic when I can't find why she's making this horrifying noise. I place my hands beneath her head and lift her face towards mine. Her eyes are all white and completely void of life. Her mouth grows wider as she screeches and her jaw begins to unhinge. Her teeth grow into long silver  spikes that pierce her lips as they grow, finally her head tears in half completely. \n",
      " Once everything was silent and this 'girl' lay dead on the ground, I grew calm with the thought that it wasn't me that had to suffer. \n",
      " Then I woke up. TLDR:series of unrelated fucked up things reveals that I'm a very selfish person.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(shift_labels[3, mask[3]]))\n",
    "print(text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}