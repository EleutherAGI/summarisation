{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "import transformers\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset json (/home/kip/.cache/huggingface/datasets/json/default-80e5db725cf45937/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514)\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset('json', field='data', data_files='./data/comparisons-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-2cb0261ccb6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m tokenized_datasets = datasets.map(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 286\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    287\u001b[0m                 k: dataset.map(\n\u001b[1;32m    288\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m         return DatasetDict(\n\u001b[1;32m    286\u001b[0m             {\n\u001b[0;32m--> 287\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    288\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mtest_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mupdate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoes_function_return_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing finished, running the mapping function on the dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdoes_function_return_dict\u001b[0;34m(inputs, indices)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mfn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             processed_inputs = (\n\u001b[0;32m-> 1211\u001b[0;31m                 \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m             )\n\u001b[1;32m   1213\u001b[0m             \u001b[0mdoes_return_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-2cb0261ccb6e>\u001b[0m in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mCaptureLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_logger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m#print(examples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtext_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' TLDR:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summaries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moutput_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput_0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        #print(examples)\n",
    "        text_0 = examples['info']['post'] + ' TLDR:' + examples['summaries'][0]['text']\n",
    "        output_0 = tokenizer(text_0, return_length = True)\n",
    "        output_0[\"total_length\"] = output_0.pop(\"length\")\n",
    "        output_0[\"summary_length\"] = tokenizer(examples['summary'], return_length = True)['length']\n",
    "        \n",
    "        text_1 = examples['info']['post'] + ' TLDR:' + examples['summaries'][0]['text']\n",
    "        output_1 = tokenizer(text_1, return_length = True)\n",
    "        output_1[\"total_length\"] = output_1.pop(\"length\")\n",
    "        output_1[\"summary_length\"] = tokenizer(examples['summary'], return_length = True)['length']\n",
    "        \n",
    "    # clm input could be much much longer than block_size\n",
    "    if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "        tok_logger.warning(\n",
    "            \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "        )\n",
    "    return [output_0, output_1]\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    #remove_columns=column_names,\n",
    "    #load_from_cache_file=not data_args.overwrite_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info TLDR:info',\n",
       " 'split TLDR:split',\n",
       " 'summaries TLDR:summaries',\n",
       " 'choice TLDR:choice',\n",
       " 'worker TLDR:worker',\n",
       " 'batch TLDR:batch',\n",
       " 'extra TLDR:extra']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[example['info']['post'] + ' TLDR:' + example['summaries'][0]['text'] for example in examples]#zip(examples['info']['post'], examples['summaries'][0]['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b6e225c23b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "text = [content + ' TLDR:' + summary for content, summary in zip(examples['info']['post'], examples['summary'])]\n",
    "output = tokenizer(text, return_length = True)\n",
    "output[\"total_length\"] = output.pop(\"length\")\n",
    "output[\"summary_length\"] = tokenizer(examples['summary'], return_length = True)['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"post: {examples['info']['post']}\")\n",
    "print(f\"summary 0: {examples['summaries'][0]['text']}\")\n",
    "print(f\"summary 1: {examples['summaries'][1]['text']}\")\n",
    "print(f\"choice: post {examples['choice']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Now I'm stuck in the double bed across from hers talking to all of you saying that if any distance grows between continue to communicate, because no communication will always kill a relationship.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples['summaries'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = datasets[\"test\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    with CaptureLogger(tok_logger) as cl:\n",
    "        text = [content + ' TLDR:' + summary for content, summary in zip(examples['content'], examples['summary'])]\n",
    "        output = tokenizer(text, return_length = True)\n",
    "        \n",
    "        summary_lengths = tokenizer(examples['summary'], return_length = True)['length']\n",
    "        total_lengths = output.pop(\"length\")\n",
    "        # for item in tokenize batch\n",
    "        output['mask'] = []\n",
    "        for i in range(len(summary_lengths)):\n",
    "            output['mask'].append([0 for _ in range(total_lengths[i])])\n",
    "            for j in range(total_lengths[i] - summary_lengths[i], total_lengths[i]):\n",
    "                output['mask'][i][j] = 1\n",
    "    # clm input could be much much longer than block_size\n",
    "    if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "        tok_logger.warning(\n",
    "            \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\"\n",
    "        )\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Die wichtigsten Aussagen des Textes werden extrahiert.\\n\"# Bitte erstelle Stichpunkte für folgenden Text:\\n\"\n",
    "prompt += \"Text: Die neuartige mRNA-Technologie habe »dramatisches Potenzial«, und Pfizer sei zuversichtlich, nun auch allein mRNA-Vakzinen entwickeln zu können, sagte Bourla laut der auf der Internetseite der Zeitung verbreiteten redaktionellen Fassung. »Wir haben unsere eigene Expertise entwickelt.« Bei Covid-19-Impfstoffen würden die beiden Unternehmen weiter kooperieren.\\n\"\n",
    "prompt += \"Aussagen:\\nBourla hat gesagt, dass mRNA-Technologie dramatisches Potenzial habe. Sie würden weiter kooperieren.\\n\"\n",
    "prompt += \"Text: \"+\"b\"+\"\\n\"\n",
    "prompt += \"Aussagen:\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die wichtigsten Aussagen des Textes werden extrahiert.\\nText: Die neuartige mRNA-Technologie habe »dramatisches Potenzial«, und Pfizer sei zuversichtlich, nun auch allein mRNA-Vakzinen entwickeln zu können, sagte Bourla laut der auf der Internetseite der Zeitung verbreiteten redaktionellen Fassung. »Wir haben unsere eigene Expertise entwickelt.« Bei Covid-19-Impfstoffen würden die beiden Unternehmen weiter kooperieren.\\nAussagen:\\nBourla hat gesagt, dass mRNA-Technologie dramatisches Potenzial habe. Sie würden weiter kooperieren.\\nText: b\\nAussagen:\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 1024\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    \n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "python midtune_mask.py \\\n",
    "    --model_name_or_path distilgpt2 \\\n",
    "    --train_file ./data/tldr-filtered-train.json \\\n",
    "    --validation_file ./data/tldr-filtered-test.json \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir ./models/distilgpt2_masked \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup deepspeed --num_gpus=6 midtune.py \\\n",
    "--deepspeed ds_config.json \\\n",
    "--model_name_or_path gpt2-xl \\\n",
    "--train_file ./data/tldr-filtered-train.json \\\n",
    "--validation_file ./data/tldr-filtered-test.json \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--fp16 \\\n",
    "--overwrite_cache \\\n",
    "--evaluation_strategy=\"steps\" \\\n",
    "--output_dir ./models/gpt2-xl \\\n",
    "--eval_steps 200 \\\n",
    "--save_steps 200 \\\n",
    "--logging_steps 25 \\\n",
    "--num_train_epochs 2 \\\n",
    "--gradient_accumulation_steps 8 \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--per_device_eval_batch_size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "8\n",
      "11\n",
      "14\n",
      "17\n",
      "20\n",
      "23\n",
      "26\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "acc_steps = 3\n",
    "\n",
    "for i in range(30):\n",
    "    if (i + 1) % acc_steps == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros_like(shift_labels, dtype=torch.bool)\n",
    "for i, (s, t) in enumerate(zip(summary_length, total_length)):\n",
    "        mask[i][t - s - 1 : t - 1] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34094,  3454,  2194,  1869,    13])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_labels[0, mask[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series of unrelated fucked up things reveals that I'm a very selfish person.\n",
      "Long one, \n",
      " I was in Walmart, being chased by a man whom I knew was going to rape me/cause me serious bodily harm. Said Walmart became a maze and was increasingly hard to navigate, then I come upon my ex boyfriend in the yarn section (which I'm not sure exists in Walmart stores) and he refused to help me. He said, \"You deserve what is happening to you\". So I'm running out of breath, feeling terrified, knowing I'm going to be caught when I see the door and run outside. \n",
      " It's raining, and very dark out. Two of my professors are out in the parking lot, one is crying and staring off into the distance. I follow her gaze to see this terrible scene: A bridge over a gaping ravine has collapsed and cars are still driving off the edge and crashing into the river beneath. People are dying right before my eyes, screaming, crying. My ears are filled with a terrible screeching sound louder than any other sobbing and I'm suddenly underneath the bridge looking up. \n",
      " I turn around and there's a little girl laying on the river-bank, she's making the screeching sound. I can see blood everywhere but begin to panic when I can't find why she's making this horrifying noise. I place my hands beneath her head and lift her face towards mine. Her eyes are all white and completely void of life. Her mouth grows wider as she screeches and her jaw begins to unhinge. Her teeth grow into long silver  spikes that pierce her lips as they grow, finally her head tears in half completely. \n",
      " Once everything was silent and this 'girl' lay dead on the ground, I grew calm with the thought that it wasn't me that had to suffer. \n",
      " Then I woke up. TLDR:series of unrelated fucked up things reveals that I'm a very selfish person.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(shift_labels[3, mask[3]]))\n",
    "print(text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
