{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def query_model(input_ids, model, batch_size, response_length = 32, attention_mask=None):    \n",
    "    #query = query.to(model.dtype)\n",
    "    response_tensors = []\n",
    "    tensor_shape = input_ids.shape[0]\n",
    "    for i in range(int(tensor_shape/batch_size)):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                mask = attention_mask[i*batch_size:(i+1)*batch_size].to(model.device)\n",
    "            else:\n",
    "                mask = None\n",
    "                \n",
    "            ids = input_ids[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            generation_output = model.generate(input_ids=ids,\n",
    "                                               attention_mask=mask,\n",
    "                                               max_length=input_ids.shape[1]+response_length, \n",
    "                                               do_sample=True)\n",
    "            \n",
    "        for tensor in generation_output:\n",
    "            response_tensors.append(tensor)\n",
    "            \n",
    "    output_ids = torch.stack(response_tensors)[:, tensor_shape:].to('cpu')\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        output_mask = torch.ones_like(output_ids)\n",
    "        output_mask[response_tensors == model.config.pad_token_id] = 0\n",
    "        return output_ids, output_mask\n",
    "    return output_ids\n",
    "    \n",
    "            \n",
    "#response, mask = query_model(input_ids=query_tensor['input_ids'],\n",
    "#                       #attention_mask=query_tensor['attention_mask'],\n",
    "#                       model=gpt2_model, \n",
    "#                       batch_size=1, \n",
    "#                       response_length = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['v_head.summary.weight', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'v_head.summary.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['v_head.summary.weight', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'v_head.summary.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "#from ppo import PPO\n",
    "from datasets import load_dataset\n",
    "from utils import logprobs_from_logits\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from gpt2withvaluehead import GPT2HeadWithValueModel, respond_to_batch\n",
    "from ppo2 import PPO\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"../models/distilgpt2_for_generation_for_scoring\",\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ").to('cuda')\n",
    "\n",
    "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(\n",
    "    \"distilgpt2\",\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ").to('cuda')\n",
    "\n",
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained(\n",
    "   \"distilgpt2\",\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    ).to('cuda')\n",
    "\n",
    "#value_model = GPT2HeadWithValueModel.from_pretrained(\n",
    "#    \"distilgpt2\"\n",
    "#).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset json (/home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514)\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-aed1fca3ca3985d6.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-75636440fa93c637.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-2ef78ea2351df0dc.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-141a3b27954a7673.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-bf64711251cb4535.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-4bde412808188a79.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-e8a2c22890cacb63.arrow\n",
      "Loading cached processed dataset at /home/kip/.cache/huggingface/datasets/json/default-913b4a67787bf3b8/0.0.0/70d89ed4db1394f028c651589fcab6d6b28dddcabbe39d3b21b4d41f9a708514/cache-1310eab8c770c143.arrow\n"
     ]
    }
   ],
   "source": [
    "#value_model = torch.nn.DataParallel(value_model, device_ids=[5,6,7])\n",
    "#value_model.to(f'cuda:{value_model.device_ids[0]}')\n",
    "\n",
    "datasets = load_dataset(\"json\", field='data', data_files={\n",
    "    \"train\": \"../data/tldr-filtered-test.json\",\n",
    "})\n",
    "\n",
    "# prep dataset\n",
    "def tokenize_function(examples):\n",
    "    text = [f'SUBREDDIT: r/{subreddit}\\nTITLE: {title}\\nPOST: {post}\\nTL;DR:' for subreddit, title, post in zip(\n",
    "        examples['subreddit'], \n",
    "        examples['title'], \n",
    "        examples['content'],)]\n",
    "    output = tokenizer(text, max_length=32, truncation=True, padding=True)\n",
    "    #output[\"total_length\"] = output.pop(\"length\")\n",
    "    #output[\"summary_length\"] = tokenizer(examples['summary'], return_length = True)['length']\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns = datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "ppo_steps = 1000000\n",
    "batch_size=64 # Should be 64\n",
    "per_device_batch_size=8\n",
    "response_len = 32\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    return tokenizer.pad(batch, return_tensors='pt')\n",
    "\n",
    "loader = DataLoader(tokenized_datasets['train'], batch_size=batch_size, pin_memory=False, collate_fn=collate_wrapper, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdog\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.24<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ruby-wood-37</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kdog/transformer_ppo\" target=\"_blank\">https://wandb.ai/kdog/transformer_ppo</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/kdog/transformer_ppo/runs/2mhtb5hw\" target=\"_blank\">https://wandb.ai/kdog/transformer_ppo/runs/2mhtb5hw</a><br/>\n",
       "                Run data is saved locally in <code>/home/kip/Desktop/summarisation/ppo/wandb/run-20210706_013809-2mhtb5hw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"cls_model_name\": \"lvwerra/bert-imdb\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 51200,\n",
    "    \"batch_size\": 64,\n",
    "    \"forward_batch_size\": 1,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 5,\n",
    "    \"txt_out_len\": 20,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "    \n",
    "#ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
    "import wandb\n",
    "wandb.init(project='transformer_ppo')\n",
    "\n",
    "ppo_trainer = PPO(model=gpt2_model, ref_model=gpt2_model_ref, batch_size=2, wandb = wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 80/632 [12:39<1:27:23,  9.50s/it]\n"
     ]
    }
   ],
   "source": [
    "fbs = 32\n",
    "#r = []\n",
    "\n",
    "for idx, l in enumerate(tqdm(loader)):\n",
    "    query_tensors = l['input_ids'].to('cuda')\n",
    "\n",
    "    response_tensors = []\n",
    "    for i in range(int(query_tensors.shape[0]/fbs)):\n",
    "        response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],\n",
    "                                     txt_len=32)\n",
    "        response_tensors.append(response)\n",
    "    response_tensors = torch.cat(response_tensors)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    scores_tensors = []\n",
    "    inputs = torch.cat((query_tensors, response_tensors), axis=1)\n",
    "    for i in range(int(query_tensors.shape[0]/fbs)):\n",
    "        response = sentiment_model(inputs[i*fbs:(i+1)*fbs])['logits'].detach()\n",
    "        scores_tensors.append(response)\n",
    "    scores_tensors = torch.squeeze(torch.cat(scores_tensors))\n",
    "    \n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, scores_tensors)\n",
    "    \n",
    "\n",
    "    data = [[tokenizer.decode(query), \n",
    "             tokenizer.decode(response)] for query, response in zip(query_tensors[:8], response_tensors[:8])]\n",
    "    wandb.log({\n",
    "        #\"model_scores\": wandb.histogram(scores_tensors.cpu()),\n",
    "        \"text_examples\": wandb.Table(data=data, columns=[\"Prompt\", \"Summary\"])\n",
    "    })\n",
    "    \n",
    "    if idx == 80:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
