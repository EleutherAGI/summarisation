import time
import torch
import torch.nn as nn
from torch.optim import Adam
from utils import *

class PPO:
    def __init__(self, 
                model: nn.Module, 
                ref_model: nn.Module, 
                value_model: nn.Module = None, 
                lr: float = 1.5e-5,
                kl_coef: float = 0.05, 
                batch_size: int = 16, 
                accumulation_steps: int = 1,
                epochs: int = 4, 
                cliprange: float = 0.2,
                cliprange_value: float = 0.2, 
                value_coef: float = 0.1,
                lambda_coef: float = 0.95,
                gamma_coef: float = 1.0,
                wandb = None):
        """Initalize the ppo algorithm

        Args:
            model (nn.Module): generation model
            ref_model (nn.Module): frozen reference model used to compute KL penalty
            value_model (nn.Module, optional): value model. Defaults to None.
            lr (float, optional): learning rate. Defaults to 1.5e-5.
            kl_coef (float, optional): KL penalty coef. Defaults to 0.05.
            batch_size (int, optional): The batch size used to train model. Defaults to 16.
            accumulation_steps (int, optional): The gradient accumulation steps used to train model. Defaults to 1.
            epochs (int, optional): Number of epochs to train ppo for. Defaults to 4.
            cliprange (float, optional): range for clipping for ppo policy_gradient loss. Defaults to 0.2.
            cliprange_value (float, optional): range for clipping values in loss calculation. Defaults to 0.2.
            value_coef (float, optional): value loss scaling coefficient. Defaults to 0.1.
            lambda_coef (float, optional): Lambda parameter for advantage calcualation. Defaults to 0.95.
            gamma_coef (float, optional): Gamma parameter for advantage calculation. Defaults to 1.0.

        """

        self.model = model
        self.ref_model = ref_model
        
        self.optimizer = Adam(model.parameters(), lr=lr)

        self.kl_coef = kl_coef
        self.batch_size = batch_size
        self.accumulation_steps = accumulation_steps
        self.epochs = epochs
        self.cliprange = cliprange
        self.cliprange_value = cliprange_value
        self.value_coef = value_coef
        self.lambda_coef = lambda_coef
        self.gamma_coef = gamma_coef
        self.wandb = wandb

        if self.wandb is not None:
            config = wandb.config
            config.kl_coef = self.kl_coef
            config.batch_size = self.batch_size
            config.epochs = self.epochs
            config.cliprange = self.cliprange
            config.cliprange_value = self.cliprange_value
            config.value_coef = self.value_coef
            config.lambda_coef = self.lambda_coef
            config.gamma_coef = self.gamma_coef


    def step(self, 
            query: torch.tensor, 
            response: torch.tensor, 
            scores: torch.tensor):
        """Run ppo step on a batch of rollouts

        Args:
            query (torch.tensor): queries provided to the model, shape [batch_size, query_length]
            reponse (torch.tensor): responses from generated by the model, shape [batch_size, response_length]
            scores (torch.tensor): scores for each generated peice of text, shape [batch_size]
        """

        rollout_size, gen_len = response.shape[:2]
        model_input = torch.cat((query, response), axis=1)

        assert rollout_size % self.batch_size == 0, "rollout size must be divisible by batch size!"

        logprobs, ref_logprobs, values = self.batched_forward_pass(model_input, gen_len)

        rewards, non_score_rewards = self.compute_rewards(scores, logprobs, ref_logprobs)

        #Calculate advantages
        lastgaelam = 0
        advantages_reversed = []
        for t in reversed(range(gen_len)):
            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
            delta = rewards[:, t] + self.gamma_coef * nextvalues - values[:, t]
            lastgaelam = delta + self.gamma_coef * self.lambda_coef * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

        # get returns
        returns = advantages + values

        # normalize the advantages
        advantages = whiten(advantages).detach()

        entropy = []
        aproxkl = []
        policykl = []
        pg_clipfrac = []
        vf_clipfrac = []

        for _ in range(self.epochs):
            self.optimizer.zero_grad()
            shuffled = torch.randperm(rollout_size).reshape(-1, self.batch_size)
            for idx, batch_idx in enumerate(shuffled):
                loss_p, loss_v, ent, stats = self.loss(old_logprobs = logprobs[batch_idx], 
                                            values = values[batch_idx], 
                                            returns = returns[batch_idx], 
                                            advantages = advantages[batch_idx], 
                                            model_input = model_input[batch_idx],
                                            gen_len = gen_len)

                entropy.append(stats['entropy'])
                aproxkl.append(stats['aproxkl'])
                policykl.append(stats['policykl'])
                pg_clipfrac.append(stats['pg_clipfrac'])
                vf_clipfrac.append(stats['vf_clipfrac'])

                loss = loss_p - ent * .01 + loss_v
                (loss / self.accumulation_steps).backward()

                if (idx + 1) % self.accumulation_steps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()

        self.wandb.log({
           "rollout_returns": self.wandb.Histogram(returns.cpu()),
           "rollout_values": self.wandb.Histogram(values.cpu()),
           "rollout_returns_mean": torch.mean(returns).cpu(),
           "rollout_values_mean": torch.mean(values).cpu(),
           "non_score_rewards": self.wandb.Histogram(non_score_rewards.cpu()),
           "rewards": torch.mean(rewards).cpu(),
           "model_scores": torch.mean(scores).cpu(), 
           "entropy": torch.mean(torch.tensor(entropy)),
           "aproxkl": torch.mean(torch.tensor(aproxkl)),
           "policykl": torch.mean(torch.tensor(policykl)),
           "pg_clipfrac": torch.mean(torch.tensor(pg_clipfrac)),
           "vf_clipfrac": torch.mean(torch.tensor(vf_clipfrac))
        })

    def compute_rewards(self, scores, logprobs, ref_logprobs):
        """Compute per token rewards from scores and KL-penalty."""
        kl = logprobs - ref_logprobs
        non_score_reward = -self.kl_coef * kl
        rewards = non_score_reward.clone().detach()
        rewards[:, -1] += scores
        return rewards, non_score_reward

    def loss(self, old_logprobs, values, returns, advantages, model_input, gen_len):
        """Calculate policy and value losses."""
        # Might try and add entropy to increase exploration
        # Also might try gradient clipping

        # Get the predicted value
        logits, _, vpred = self.model(model_input)
        logprob = logprobs_from_logits(logits[:,:-1,:], model_input[:, 1:])
        
        #only the generation part of the values/logprobs is needed
        logprob, vpred = logprob[:, -gen_len:], vpred[:,-gen_len-1:-1]

        # Clip the value to reduce variability during Critic training
        vpredclipped = values + torch.clamp(vpred - values,
                                               -self.cliprange_value,
                                               self.cliprange_value)
        # Unclipped value
        vf_losses1 = torch.square(vpred - returns)
        # Clipped value
        vf_losses2 = torch.square(vpredclipped - returns)

        # Final VF loss
        vf_loss = .5 * torch.mean(torch.max(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = torch.exp(logprob - old_logprobs)
        
        # Defining Loss = - J is equivalent to max J
        pg_losses = -advantages * ratio

        pg_losses2 = -advantages * torch.clamp(ratio,
                                               1.0 - self.cliprange,
                                               1.0 + self.cliprange)
        # Final PG loss
        pg_loss = torch.mean(torch.max(pg_losses, pg_losses2))

        entropy = torch.mean(entropy_from_logits(logits))

        stats = {
            "entropy": entropy.cpu().item(),
            "aproxkl": .5 * torch.mean((logprob - old_logprobs)**2).cpu().item(),
            "policykl": torch.mean(logprob - old_logprobs).cpu().item(),
            "pg_clipfrac" : torch.mean(torch.gt(pg_losses2, pg_losses).double()).cpu().item(),
            "vf_clipfrac":  torch.mean(torch.gt(vf_losses2, vf_losses1).double()).cpu().item(),
        }

        return pg_loss, self.value_coef * vf_loss, entropy, stats

        
    def batched_forward_pass(self, model_input, gen_len):
        """Calculate model outputs in multiple batches."""
        rollout_size = model_input.shape[0]
        logprobs = []
        ref_logprobs = []
        values = []
        
        for i in range(rollout_size//self.batch_size):
            m_input = model_input[i*self.batch_size:(i+1)*self.batch_size]
            logits, _, v = self.model(m_input)
            ref_logits, _, _ = self.ref_model(m_input)
            
            values.append(v[:, -gen_len-1:-1].detach())
            logprobs.append(logprobs_from_logits(logits[:,:-1,:], m_input[:,1:])[:, -gen_len:].detach())
            ref_logprobs.append(logprobs_from_logits(ref_logits[:,:-1,:], m_input[:,1:])[:, -gen_len:].detach())

        return torch.cat(logprobs), torch.cat(ref_logprobs), torch.cat(values)