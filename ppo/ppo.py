import torch
from torch import nn
import torch.nn.functional as F

from constants import * # CAPITAL vars are constants
import util

# Take CLIP model, text generated by model and
# critic (review text)
def clip_reward(clip_model, generated_tokens, critic):
    passage_embedding = clip_model.encodeX(generated_tokens).squeeze()
    review_embedding = clip_model.encodeY(critic).squeeze()

    sim = F.consine_similarity(passage_embedding, review_embedding)
    # Reward on [-1, 1] should be good
    return sim

def kl_reward(new_log_probs, baseline_log_probs):
    kl_divs = (new_log_probs - old_log_probs) * torch.exp(new_log_probs)
    kl_divs = torch.sum(kl_divs, dim = 1)
    return kl_divs.mean()

# PPO loss functions
def policy_loss(model, state_batch, action_batch, action_log_probs, adv_batch):
    pi, _, _ = model(state_batch)

    new_log_probs = pi.log_prob(action_batch)

    ratio = (new_log_probs - action_log_probs).exp()
    surr1 = ratio * adv_batch
    surr2 = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * adv_batch

    loss = - torch.min(surr1, surr2).mean()
    ent_loss = pi.entropy().mean()

    return loss + ENT_COEFF * ent_loss

def value_loss(model, state_batch, target_batch):
    _, value, _ = model(state_batch)
    return (value - target_batch).pow(2).mean()

# Runs a single train stage over rollout
def train_PPO(model, opt, rollout_store):
    rollout_store.cuda()
    rollout_store.detach()

    size = rollout_store.size
    
    targets, advantages = util.get_targets_and_advs(
            rollout_store.get('reward'),
            rollout_store.get('value'))

    size = rollout_store.size
    total_loss = 0

    model.train()
    for epoch in range(EPOCHS_PER_ROLLOUT):
        # Shuffle indices
        if BATCH_SIZE == -1:
            inds = [torch.randperm(size)]
        else:
            inds = util.generate_indices(size, BATCH_SIZE)
        for ind in inds:
            state_batch = rollout_store.store['state'][ind]
            act_batch = rollout_store.store['action'][ind]
            log_prob_batch = rollout_store.store['log_prob'][ind]
            reward_batch = rollout_store.store['reward'][ind]

            actor_loss = policy_loss(model, state_batch, act_batch,
                    log_prob_batch, advantages[ind])
            critic_loss = value_loss(model, state_batch, targets[ind])
            loss = actor_loss + CRITIC_COEFF * critic_loss
            
            opt.zero_grad()
            loss.backward()
            opt.step()

            total_loss += loss.item()
            
    return total_loss