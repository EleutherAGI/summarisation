import torch
import torch.nn as nn
import torch.optim as optim
from utils import *

class PPO:
    def __init__(self, 
                model: nn.Module, 
                ref_model: nn.Module, 
                value_model: nn.Module,
                optimizer: optim.Optimizer = None, 
                lr: float = 1.5e-5,
                kl_coef: float = 0.05, 
                batch_size: int = 1, 
                forward_batch_size: int = 16, 
                accumulation_steps: int = 1,
                epochs: int = 4, 
                cliprange: float = 0.2,
                cliprange_value: float = 0.2, 
                value_coef: float = 0.1,
                lambda_coef: float = 0.95,
                gamma_coef: float = 1.0,
                wandb = None):
        """Initalize the ppo algorithm

        Args:
            model (nn.Module): generation model
            ref_model (nn.Module): frozen reference model used to compute KL penalty
            value_model (nn.Module, optional): value model. Defaults to None.
            lr (float, optional): learning rate. Defaults to 1.5e-5.
            kl_coef (float, optional): KL penalty coef. Defaults to 0.05.
            batch_size (int, optional): The batch size used to train model. Defaults to 16.
            accumulation_steps (int, optional): The gradient accumulation steps used to train model. Defaults to 1.
            epochs (int, optional): Number of epochs to train ppo for. Defaults to 4.
            cliprange (float, optional): range for clipping for ppo policy_gradient loss. Defaults to 0.2.
            cliprange_value (float, optional): range for clipping values in loss calculation. Defaults to 0.2.
            value_coef (float, optional): value loss scaling coefficient. Defaults to 0.1.
            lambda_coef (float, optional): Lambda parameter for advantage calcualation. Defaults to 0.95.
            gamma_coef (float, optional): Gamma parameter for advantage calculation. Defaults to 1.0.

        """
        self.value_model = value_model

        self.model = model
        self.ref_model = ref_model
        
        if optimizer is None:
            self.optimizer = optim.Adam(list(self.value_model.parameters()) + list(self.model.parameters()), lr=lr)
        else:
            self.optimizer = optimizer

        self.kl_coef = kl_coef
        self.batch_size = batch_size
        self.forward_batch_size = forward_batch_size
        self.accumulation_steps = accumulation_steps
        self.epochs = epochs
        self.cliprange = cliprange
        self.cliprange_value = cliprange_value
        self.value_coef = value_coef
        self.lambda_coef = lambda_coef
        self.gamma_coef = gamma_coef
        self.wandb = wandb

        if self.wandb is not None:
            config = wandb.config
            config.kl_coef = self.kl_coef
            config.batch_size = self.batch_size
            config.epochs = self.epochs
            config.cliprange = self.cliprange
            config.cliprange_value = self.cliprange_value
            config.value_coef = self.value_coef
            config.lambda_coef = self.lambda_coef
            config.gamma_coef = self.gamma_coef

    def step(self, 
            model_input: torch.tensor, 
            scores: torch.tensor,
            mask: torch.tensor,):
        """Run ppo step on a batch of rollouts

        Args:
            model_input (torch.tensor): queries provided to the model, shape [batch_size, input_length]
            scores (torch.tensor): scores for each generated peice of text, shape [batch_size]
            mask (torch.tensor): mask for each input generated by model(should not include padding tokens), shape [batch_size, input_length]
        """

        rollout_size = model_input.shape[0]
        #model_input = torch.cat((query, response), axis=1)

        # Make mask
        mask = mask[:, 1:]
        #mask = torch.ne(model_input[:, -gen_len:], self.model.config.pad_token_id).to(self.model.device)

        assert rollout_size % self.batch_size == 0, "rollout size must be divisible by batch size!"

        logprobs, ref_logprobs, values = self.batched_forward_pass(model_input)

        #move values to main model device
        values = values.to(self.model.device)
        
        #calculate rewards
        rewards, non_score_rewards = self.calculate_rewards(scores, logprobs, ref_logprobs, mask)

        #Calculate advantages
        advantages = self.calculate_advantages(values, rewards, mask)

        # get returns
        returns = advantages + values

        # normalize the advantages
        advantages = whiten(advantages, mask)

        entropy = []
        aproxkl = []
        policykl = []
        pg_clipfrac = []
        vf_clipfrac = []

        for _ in range(self.epochs):
            self.optimizer.zero_grad()
            shuffled = torch.randperm(rollout_size).reshape(-1, self.batch_size)
            for idx, batch_idx in enumerate(shuffled):
                loss_p, loss_v, stats = self.loss(old_logprobs = logprobs[batch_idx], 
                                            values = values[batch_idx], 
                                            returns = returns[batch_idx], 
                                            advantages = advantages[batch_idx], 
                                            model_input = model_input[batch_idx],
                                            mask = mask[batch_idx])

                entropy.append(stats['entropy'])
                aproxkl.append(stats['aproxkl'])
                policykl.append(stats['policykl'])
                pg_clipfrac.append(stats['pg_clipfrac'])
                vf_clipfrac.append(stats['vf_clipfrac'])

                loss = loss_p + loss_v

                (loss / self.accumulation_steps).backward()

                #if (idx + 1) % self.accumulation_steps == 0:
                #    self.optimizer.step()
                #    self.optimizer.zero_grad()

        if self.wandb is not None:
            self.wandb.log({
                "rollout_returns": self.wandb.Histogram(returns.cpu()),
                "rollout_values": self.wandb.Histogram(values.cpu()),
                "rollout_returns_mean": torch.mean(returns).cpu(),
                "rollout_values_mean": torch.mean(values).cpu(),
                "non_score_rewards": self.wandb.Histogram(non_score_rewards.cpu()),
                "rewards": torch.mean(rewards).cpu(),
                "model_scores": torch.mean(scores).cpu(), 
                "entropy": torch.mean(torch.tensor(entropy)),
                "aproxkl": torch.mean(torch.tensor(aproxkl)),
                "policykl": torch.mean(torch.tensor(policykl)),
                "pg_clipfrac": torch.mean(torch.tensor(pg_clipfrac)),
                "vf_clipfrac": torch.mean(torch.tensor(vf_clipfrac))
            })

    def calculate_advantages(self, values, rewards, mask):
        """Generalized Advantage Estimation calculation"""
        length = values.shape[1]
        lastgaelam = 0
        advantages_reversed = []
        for t in reversed(range(length)):
            nextvalues = values[:, t + 1] * mask[:, t + 1] if t < length - 1 else 0.0
            delta = rewards[:, t] + self.gamma_coef * nextvalues - values[:, t] * mask[:, t]
            lastgaelam = delta + self.gamma_coef * self.lambda_coef * mask[:, t] * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1).detach()
        return advantages

    def calculate_rewards(self, scores, logprobs, ref_logprobs, mask):
        """Compute per token rewards from scores and KL-penalty."""
        kl = logprobs - ref_logprobs.to(self.model.device)
        non_score_reward = -self.kl_coef * kl
        rewards = non_score_reward.clone().detach() * mask

        # Using generation mask we get the last kl that was generated
        # and increment it using the scores
        generated_tokens = [torch.where(item==True)[0] for item in mask]
        last_generated_tokens = [item[item.shape[0]-1] if item.shape[0] != 0 else torch.tensor(0) for item in generated_tokens]
        rewards[range(rewards.shape[0]), last_generated_tokens] += scores.to(self.model.device)
        return rewards, non_score_reward

    def loss(self, old_logprobs, values, returns, advantages, model_input, mask):
        """Calculate policy and value losses."""
        # Might try and add entropy to increase exploration
        # Also might try gradient clipping

        # Mask parts that are not generated by the model.
        old_logprobs, values, returns, advantages = old_logprobs[mask], values[mask], returns[mask], advantages[mask]

        # Get the predicted value
        logits = self.get_logits(self.model, model_input)
        vpred = self.get_values(self.value_model, model_input)[:,:-1].to(self.model.device)

        logprob = logprobs_from_logits(logits[:,:-1,:], model_input[:, 1:].to(self.model.device))

        #only the generation part of the values/logprobs is needed
        logprob, vpred = logprob[mask], vpred[mask]

        # Clip the value to reduce variability during Critic training
        vpredclipped = values + torch.clamp(vpred - values,
                                               -self.cliprange_value,
                                               self.cliprange_value)
        # Unclipped value
        vf_losses1 = torch.square(vpred - returns)
        # Clipped value
        vf_losses2 = torch.square(vpredclipped - returns)

        # Final VF loss
        vf_loss = .5 * torch.mean(torch.max(vf_losses1, vf_losses2))

        # Calculate ratio (pi current policy / pi old policy)
        ratio = torch.exp(logprob - old_logprobs)
        
        # Defining Loss = - J is equivalent to max J
        pg_losses1 = -advantages * ratio

        pg_losses2 = -advantages * torch.clamp(ratio,
                                               1.0 - self.cliprange,
                                               1.0 + self.cliprange)
        # Final PG loss
        pg_loss = torch.mean(torch.max(pg_losses1, pg_losses2))

        entropy = torch.mean(entropy_from_logits(logits))

        print(pg_losses1)
        print(vpredclipped)

        stats = {
            "entropy": entropy.cpu().item(),
            "aproxkl": .5 * torch.mean((logprob - old_logprobs)**2).cpu().item(),
            "policykl": torch.mean(logprob - old_logprobs).cpu().item(),
            "pg_clipfrac" : torch.mean(torch.gt(pg_losses2, pg_losses1).double()).cpu().item(),
            "vf_clipfrac":  torch.mean(torch.gt(vf_losses2, vf_losses1).double()).cpu().item(),
        }

        return pg_loss, self.value_coef * vf_loss, stats
  
    def batched_forward_pass(self, model_input):
        """Calculate model outputs in multiple batches."""
        rollout_size = model_input.shape[0]
        logprobs = []
        ref_logprobs = []
        values = []
        
        for i in range(rollout_size//self.forward_batch_size):
            m_input = model_input[i*self.forward_batch_size:(i+1)*self.forward_batch_size]
            
            with torch.no_grad():
                logits = self.get_logits(self.model, m_input)
                ref_logits = self.get_logits(self.ref_model, m_input)
                v = self.get_values(self.value_model, m_input)
                
                values.append(v[:, :-1].detach())
                logprobs.append(logprobs_from_logits(logits[:,:-1,:], 
                        m_input[:,1:].to(self.model.device)).detach())

                ref_logprobs.append(logprobs_from_logits(ref_logits[:,:-1,:], 
                        m_input[:,1:].to(self.ref_model.device)).detach())


        return torch.cat(logprobs), torch.cat(ref_logprobs), torch.cat(values)

    def get_values(self, model, input_tokens):
        """Get values from value model"""
        mask = torch.ne(input_tokens, model.config.pad_token_id).long().to(model.device)
        model_inputs = model.prepare_inputs_for_generation(input_tokens.to(model.device), 
                                                            attention_mask = mask)
        values = torch.squeeze(model(**model_inputs)['logits'], dim=-1)
        return values

    def get_logits(self, model, input_tokens):
        """Get logits from generation model"""
        mask = torch.ne(input_tokens, model.config.pad_token_id).long().to(model.device)
        model_inputs = model.prepare_inputs_for_generation(input_tokens.to(model.device), 
                                                            attention_mask = mask)
        logits = model(**model_inputs)['logits']
        return logits
